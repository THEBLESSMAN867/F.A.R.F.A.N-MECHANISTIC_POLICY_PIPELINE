"""
REAL calibration test for the 30 executors (D1Q1-D6Q5).

This test ACTUALLY calibrates executor methods using the real orchestrator,
verifying that:
1. All 30 executors are calibrated
2. Each executor uses ALL 8 layers
3. Base layer loads from intrinsic_calibration.json
4. Final scores are computed correctly
"""
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.saaaaaa.core.calibration.orchestrator import CalibrationOrchestrator
from src.saaaaaa.core.calibration.intrinsic_loader import IntrinsicScoreLoader
from src.saaaaaa.core.calibration.layer_requirements import LayerRequirementsResolver
from src.saaaaaa.core.calibration.data_structures import ContextTuple, LayerID
from src.saaaaaa.core.calibration.pdt_structure import PDTStructure


def create_minimal_pdt():
    """Create a minimal PDT structure for testing."""
    # This is a mock - in real usage, PDT would be parsed from actual document
    return PDTStructure(
        full_text="Plan de Desarrollo Territorial 2024-2027. Diagn√≥stico. Parte Estrat√©gica. PPI. Seguimiento.",
        total_tokens=1000,
        blocks_found={
            "Diagn√≥stico": {"text": "Contexto municipal", "tokens": 200, "numbers_count": 5},
            "Parte Estrat√©gica": {"text": "Objetivos y metas", "tokens": 300, "numbers_count": 10},
            "PPI": {"text": "Plan Plurianual de Inversiones", "tokens": 300, "numbers_count": 50},
            "Seguimiento": {"text": "Indicadores de seguimiento", "tokens": 200, "numbers_count": 15}
        },
        indicator_matrix_present=True,
        ppi_matrix_present=True
    )


def create_test_context(question_id="Q001"):
    """Create a minimal context for testing."""
    return ContextTuple(
        question_id=question_id,
        dimension="DIM01",
        policy_area="PA01",
        unit_quality=0.75  # Pre-computed PDT quality score
    )


def test_executor_calibration():
    """Test that all 30 executors can be calibrated with 8 layers."""
    print("=" * 80)
    print("EXECUTOR CALIBRATION TEST - REAL END-TO-END")
    print("=" * 80)
    print()

    # Initialize system
    print("Initializing calibration system...")
    orchestrator = CalibrationOrchestrator(
        intrinsic_calibration_path="config/intrinsic_calibration.json"
    )
    print("‚úì Orchestrator initialized")
    print()

    # Prepare test data
    pdt = create_minimal_pdt()
    print("‚úì PDT structure created")
    print()

    # Test all 30 executors
    print("Testing all 30 executors:")
    print("-" * 80)

    executor_template = "src.saaaaaa.core.orchestrator.executors.D{d}Q{q}_Executor.execute"
    results = []
    failed = []

    for d in range(1, 7):  # D1 through D6
        for q in range(1, 6):  # Q1 through Q5
            executor_id = executor_template.format(d=d, q=q)
            # Create canonical question ID (Q001, Q002, etc.)
            # Map D1Q1-D6Q5 to Q001-Q030
            question_num = (d - 1) * 5 + q
            question_id = f"Q{question_num:03d}"
            context = create_test_context(question_id)

            print(f"\nCalibrating: {executor_id}")

            try:
                # REAL calibration
                result = orchestrator.calibrate(
                    method_id=executor_id,
                    method_version="1.0.0",
                    context=context,
                    pdt_structure=pdt,
                    graph_config="test",
                    subgraph_id=f"test_D{d}Q{q}"
                )

                # Verify 8 layers were evaluated
                layer_count = len(result.layer_scores)
                all_8_layers_present = layer_count == 8

                # Expected layers
                expected_layers = {
                    LayerID.BASE,
                    LayerID.UNIT,
                    LayerID.QUESTION,
                    LayerID.DIMENSION,
                    LayerID.POLICY,
                    LayerID.CONGRUENCE,
                    LayerID.CHAIN,
                    LayerID.META
                }
                actual_layers = set(result.layer_scores.keys())
                has_all_layers = expected_layers == actual_layers

                # Report
                status = "‚úì" if has_all_layers else "‚úó"
                print(f"  {status} Final score: {result.final_score:.3f}")
                print(f"  {status} Layers evaluated: {layer_count}/8")

                if has_all_layers:
                    print("  ‚úì All 8 layers present:")
                    for layer_id in sorted(actual_layers, key=lambda x: x.value):
                        score = result.layer_scores[layer_id].score
                        print(f"      {layer_id.value:10s}: {score:.3f}")
                else:
                    print(f"  ‚úó MISSING LAYERS: {expected_layers - actual_layers}")
                    failed.append(executor_id)

                results.append({
                    "executor": executor_id,
                    "score": result.final_score,
                    "layers": layer_count,
                    "has_all_8": has_all_layers
                })

            except Exception as e:
                print(f"  ‚úó FAILED: {e}")
                failed.append(executor_id)
                results.append({
                    "executor": executor_id,
                    "score": 0.0,
                    "layers": 0,
                    "has_all_8": False,
                    "error": str(e)
                })

    # Summary
    print()
    print("=" * 80)
    print("SUMMARY")
    print("=" * 80)

    successful = sum(1 for r in results if r.get("has_all_8", False))
    print(f"Total executors tested: {len(results)}")
    print(f"Successful (8 layers): {successful}")
    print(f"Failed: {len(failed)}")

    if failed:
        print("\nFailed executors:")
        for executor_id in failed:
            print(f"  - {executor_id}")

    # Statistics
    if successful > 0:
        scores = [r["score"] for r in results if r.get("has_all_8", False)]
        avg_score = sum(scores) / len(scores)
        min_score = min(scores)
        max_score = max(scores)

        print(f"\nScore statistics (successful calibrations):")
        print(f"  Average: {avg_score:.3f}")
        print(f"  Min: {min_score:.3f}")
        print(f"  Max: {max_score:.3f}")

    # Verification
    print()
    if successful == 30:
        print("‚úÖ SUCCESS: All 30 executors calibrated with 8 layers!")
        return True
    else:
        print(f"‚ö†Ô∏è  WARNING: Only {successful}/30 executors fully calibrated")
        return False


def test_executor_detection():
    """Test that executor detection works correctly."""
    print("\n" + "=" * 80)
    print("EXECUTOR DETECTION TEST")
    print("=" * 80)
    print()

    # Test cases
    test_cases = [
        ("src.saaaaaa.core.orchestrator.executors.D1Q1_Executor.execute", True),
        ("src.saaaaaa.core.orchestrator.executors.D6Q5_Executor.execute", True),
        ("src.saaaaaa.core.orchestrator.executors.D3Q2_Executor._extract", True),
        ("src.saaaaaa.processing.SomeClass.method", False),
        ("src.saaaaaa.analysis.Analyzer.analyze", False),
        ("D4Q3_Executor", True),  # Even without full path
        ("utils.format_string", False),
    ]

    all_passed = True
    for method_id, expected in test_cases:
        result = LayerRequirementsResolver.is_executor(method_id)
        status = "‚úì" if result == expected else "‚úó"
        if result != expected:
            all_passed = False
        print(f"  {status} {method_id:60s} ‚Üí {result} (expected: {expected})")

    print()
    if all_passed:
        print("‚úÖ All executor detection tests passed!")
    else:
        print("‚ö†Ô∏è  Some executor detection tests failed")

    return all_passed


def test_layer_forcing():
    """Test that executors get 8 layers forced."""
    print("\n" + "=" * 80)
    print("LAYER FORCING TEST")
    print("=" * 80)
    print()

    intrinsic_loader = IntrinsicScoreLoader("config/intrinsic_calibration.json")
    resolver = LayerRequirementsResolver(intrinsic_loader)

    # Test executor
    executor_id = "src.saaaaaa.core.orchestrator.executors.D1Q1_Executor.execute"
    executor_layers = resolver.get_required_layers(executor_id)

    print(f"Executor: {executor_id}")
    print(f"Required layers: {len(executor_layers)}")
    print(f"Layers: {sorted([l.value for l in executor_layers])}")

    if len(executor_layers) == 8:
        print("‚úÖ Executor correctly requires 8 layers!")
        executor_test = True
    else:
        print(f"‚ö†Ô∏è  Executor only requires {len(executor_layers)} layers")
        executor_test = False

    # Test non-executor (should use role-based)
    utility_id = "src.saaaaaa.utils.SomeUtil.format"
    utility_layers = resolver.get_required_layers(utility_id)

    print(f"\nUtility method: {utility_id}")
    print(f"Required layers: {len(utility_layers)}")
    print(f"Layers: {sorted([l.value for l in utility_layers])}")

    # Utility should have fewer layers (conservative = 8, but if role detected, could be 3)
    utility_test = True  # Just verify it returns something

    print()
    return executor_test and utility_test


if __name__ == "__main__":
    print("CALIBRATION SYSTEM - REAL EXECUTOR TESTS")
    print()

    # Run tests
    test1 = test_executor_detection()
    test2 = test_layer_forcing()
    test3 = test_executor_calibration()

    # Final result
    print("\n" + "=" * 80)
    print("FINAL RESULTS")
    print("=" * 80)
    print(f"Executor detection: {'‚úÖ PASS' if test1 else '‚ùå FAIL'}")
    print(f"Layer forcing: {'‚úÖ PASS' if test2 else '‚ùå FAIL'}")
    print(f"Real calibration: {'‚úÖ PASS' if test3 else '‚ùå FAIL'}")
    print()

    if test1 and test2 and test3:
        print("üéâ ALL TESTS PASSED!")
        sys.exit(0)
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED")
        sys.exit(1)
