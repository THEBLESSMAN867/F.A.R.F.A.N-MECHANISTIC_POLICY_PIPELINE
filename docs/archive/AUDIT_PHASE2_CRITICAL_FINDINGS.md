# AUDITOR√çA CR√çTICA - FLUX PHASE 2 (NORMALIZE) Y SPC QUALITY GATES
**Fecha:** 2025-11-17
**Auditor:** Claude Sonnet 4.5
**Prioridad:** üî¥ CR√çTICA - M√ÅXIMO EST√ÅNDAR REQUERIDO

---

## CONTEXTO

**SPC (Smart Policy Chunks)** es el √öNICO punto de entrada can√≥nico para ingestion (Phase-One) v√≠a `CPPIngestionPipeline` en `src/saaaaaa/processing/spc_ingestion/__init__.py`.

**FLUX** proporciona fases complementarias que procesan el output de SPC (normalize, chunk, signals, aggregate, score, report).

Esta auditor√≠a identific√≥ bugs cr√≠ticos en:
- FLUX Phase 2 (normalize): `run_normalize()` en `src/saaaaaa/flux/phases.py`
- SPC Quality Gates: `SPCQualityGates` en `src/saaaaaa/processing/spc_ingestion/quality_gates.py`
- SPC Converter: `SmartChunkConverter` en `src/saaaaaa/processing/spc_ingestion/converter.py`

---

## RESUMEN EJECUTIVO

Se identificaron **5 BUGS CR√çTICOS** que violan los principios de m√°ximo est√°ndar y representan downgrades inaceptables del sistema:

1. **üî¥ CR√çTICO**: FLUX `run_normalize()` ignora completamente `NormalizeConfig` (unicode_form, keep_diacritics)
2. **üî¥ CR√çTICO**: Normalizaci√≥n Unicode NO implementada en FLUX (comentario TODO en producci√≥n)
3. **üî¥ CR√çTICO**: Split simplista por `\n` en FLUX sin procesamiento ling√º√≠stico real
4. **üü° ALTO**: SPC SmartChunkConverter puede perder embeddings si numpy no disponible
5. **üü° MEDIO**: SPC Quality gates no validan provenance_completeness = 1.0

**NOTA**: Bug #1-3 fueron encontrados en FLUX Phase 2 (normalize), NO en SPC Phase-One que es independiente y can√≥nico.

---

## 1. BUG CR√çTICO: run_normalize() IGNORA NormalizeConfig

### Ubicaci√≥n
- **Archivo:** `src/saaaaaa/flux/phases.py`
- **L√≠nea:** 302-414
- **Funci√≥n:** `run_normalize()`

### Problema
La funci√≥n recibe `cfg: NormalizeConfig` con par√°metros:
- `unicode_form: Literal["NFC", "NFKC"] = "NFC"`
- `keep_diacritics: bool = True`

**PERO NINGUNO SE USA EN LA IMPLEMENTACI√ìN**

### C√≥digo Actual (INCORRECTO)
```python
def run_normalize(
    cfg: NormalizeConfig,  # ‚Üê Par√°metro RECIBIDO PERO IGNORADO
    ing: IngestDeliverable,
    *,
    policy_unit_id: str | None = None,
    correlation_id: str | None = None,
    envelope_metadata: dict[str, str] | None = None,
) -> PhaseOutcome:
    # ...

    # TODO: Implement actual normalization (unicode normalization, etc.)
    sentences = [s for s in ing.raw_text.split("\n") if s.strip()]
    sentence_meta: list[dict[str, Any]] = [
        {"index": i, "length": len(s)} for i, s in enumerate(sentences)
    ]
    # ...
```

### Impacto
- ‚ùå **Violaci√≥n de contrato**: Funci√≥n acepta configuraci√≥n pero la ignora
- ‚ùå **No-determinismo**: Sin normalizaci√≥n Unicode, textos con diferentes encodings producen outputs diferentes
- ‚ùå **P√©rdida de calidad**: Caracteres acentuados, ligaduras, etc. no normalizados
- ‚ùå **Fallo de auditor√≠a**: TODO en producci√≥n indica funcionalidad incompleta
- ‚ùå **Downgrade masivo**: Split por `\n` es algoritmo de complejidad m√≠nima, no m√°xima

### Evidencia de Dise√±o Correcto
En `configs.py` l√≠neas 30-46:
```python
class NormalizeConfig(BaseModel):
    """Configuration for normalize phase."""

    unicode_form: Literal["NFC", "NFKC"] = "NFC"
    keep_diacritics: bool = True

    @classmethod
    def from_env(cls) -> NormalizeConfig:
        """Create config from environment variables."""
        return cls(
            unicode_form=os.getenv("FLUX_NORMALIZE_UNICODE_FORM", "NFC"),
            keep_diacritics=os.getenv("FLUX_NORMALIZE_KEEP_DIACRITICS", "true").lower() == "true",
        )
```

**La configuraci√≥n EXISTE y est√° bien dise√±ada, pero NO SE USA.**

---

## 2. BUG CR√çTICO: Normalizaci√≥n Unicode NO Implementada

### Problema
El comentario `# TODO: Implement actual normalization (unicode normalization, etc.)` en l√≠nea 345 indica que la funcionalidad FUNDAMENTAL no est√° implementada.

### Qu√© Deber√≠a Hacer
Seg√∫n el README (l√≠nea 31):
> Unicode NFC normalization | ‚úÖ | ICU-compatible via Rust

La normalizaci√≥n Unicode deber√≠a:
1. Aplicar NFC o NFKC seg√∫n configuraci√≥n
2. Normalizar espacios en blanco (m√∫ltiples espacios ‚Üí uno, tabs ‚Üí espacios)
3. Normalizar caracteres combinados (√© puede ser U+00E9 o U+0065+U+0301)
4. Preservar o eliminar diacr√≠ticos seg√∫n `keep_diacritics`
5. Garantizar output determinista byte-a-byte

### C√≥digo Esperado (Alto Est√°ndar)
```python
import unicodedata
import re

# Normalizaci√≥n Unicode
normalized_text = unicodedata.normalize(cfg.unicode_form, ing.raw_text)

# Normalizaci√≥n de espacios
normalized_text = re.sub(r'\s+', ' ', normalized_text)  # M√∫ltiples espacios ‚Üí uno
normalized_text = re.sub(r'\t', ' ', normalized_text)  # Tabs ‚Üí espacios
normalized_text = re.sub(r' \n ', '\n', normalized_text)  # Limpiar alrededor de \n

# Eliminar diacr√≠ticos si configurado
if not cfg.keep_diacritics:
    # NFD descompone √© ‚Üí e + ÃÅ (acento)
    nfd = unicodedata.normalize('NFD', normalized_text)
    # Filtrar caracteres de combinaci√≥n (categor√≠a Mn)
    normalized_text = ''.join(c for c in nfd if unicodedata.category(c) != 'Mn')
    # Volver a componer
    normalized_text = unicodedata.normalize('NFC', normalized_text)

# Segmentaci√≥n de oraciones (NO split simplista)
# Opci√≥n 1: Usar spaCy
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp(normalized_text)
sentences = [sent.text for sent in doc.sents]

# Opci√≥n 2: Usar regex sofisticado (fallback)
import re
# Pattern que respeta abreviaturas, decimales, etc.
sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', normalized_text)
```

### Impacto
- ‚ùå **Irreproducibilidad**: Diferentes encodings ‚Üí diferentes outputs
- ‚ùå **P√©rdida de calidad ling√º√≠stica**: Split por `\n` rompe oraciones reales
- ‚ùå **Violaci√≥n de especificaci√≥n**: README promete NFC, c√≥digo no lo hace

---

## 3. BUG CR√çTICO: Split Simplista por `\n`

### C√≥digo Actual
```python
sentences = [s for s in ing.raw_text.split("\n") if s.strip()]
```

### Problemas
1. **Asume que `\n` = l√≠mite de oraci√≥n**
   - ‚ùå Falso: PDFs pueden tener `\n` por wrapping de l√≠nea, no fin de oraci√≥n
   - ‚ùå Ejemplo: "El presupuesto\npara 2024 es..." ‚Üí se divide incorrectamente

2. **No respeta puntuaci√≥n real**
   - ‚ùå "Hola. Mundo" en misma l√≠nea ‚Üí UNA oraci√≥n (incorrecto)
   - ‚ùå "Objetivo: aumentar..." ‚Üí se corta (incorrecto)

3. **No maneja casos especiales**
   - ‚ùå Abreviaturas: "Sr. Juan" ‚Üí se corta en "Sr." (incorrecto)
   - ‚ùå Listas numeradas: "1. Primero\n2. Segundo" ‚Üí cortes incorrectos

4. **Violaci√≥n del principio de m√°xima complejidad**
   - Usuario exigi√≥: "NO ENFRENTES LA COMPLEJIDAD POR EL CAMINO F√ÅCIL"
   - Split por `\n` es literalmente el camino m√°s f√°cil y menos correcto

### Soluci√≥n de Alto Est√°ndar
```python
# Usar spaCy para segmentaci√≥n real
import spacy
nlp = spacy.load("es_core_news_lg")  # Modelo grande para m√°xima precisi√≥n

# Procesar con pipeline completo
doc = nlp(normalized_text)

# Extraer oraciones con metadata rica
sentences = []
sentence_meta = []

for i, sent in enumerate(doc.sents):
    sentences.append(sent.text)

    sentence_meta.append({
        "index": i,
        "length": len(sent.text),
        "char_start": sent.start_char,
        "char_end": sent.end_char,
        "token_count": len(sent),
        "has_verb": any(token.pos_ == "VERB" for token in sent),
        "entities": [ent.text for ent in sent.ents],
        "root_lemma": sent.root.lemma_,
    })
```

---

## 4. BUG ALTO: SmartChunkConverter P√©rdida de Embeddings

### Ubicaci√≥n
- **Archivo:** `src/saaaaaa/processing/spc_ingestion/converter.py`
- **L√≠nea:** 489-494

### C√≥digo
```python
# Add embeddings if available (as lists for JSON serialization)
if hasattr(sc, 'semantic_embedding') and sc.semantic_embedding is not None:
    try:
        import numpy as np
        chunk_data['semantic_embedding'] = sc.semantic_embedding.tolist()
    except Exception:
        pass  # Skip if conversion fails ‚Üê üî¥ SILENT FAILURE
```

### Problema
- **Silent failure**: Si numpy falla o tolist() falla, se pierde el embedding SIN ADVERTENCIA
- **Violaci√≥n de provenance**: Embeddings son parte cr√≠tica de SPC rich data
- **No hay logging**: Fallo silencioso imposibilita debug

### Fix de Alto Est√°ndar
```python
# Add embeddings if available (as lists for JSON serialization)
if hasattr(sc, 'semantic_embedding') and sc.semantic_embedding is not None:
    try:
        import numpy as np
        if not isinstance(sc.semantic_embedding, np.ndarray):
            self.logger.warning(
                f"Chunk {sc.chunk_id}: semantic_embedding is not np.ndarray, got {type(sc.semantic_embedding)}"
            )
        else:
            chunk_data['semantic_embedding'] = sc.semantic_embedding.tolist()
            chunk_data['embedding_dim'] = sc.semantic_embedding.shape[0]
    except ImportError as e:
        self.logger.error(f"Chunk {sc.chunk_id}: Failed to import numpy: {e}")
        raise RuntimeError("NumPy is required for embedding preservation") from e
    except Exception as e:
        self.logger.error(f"Chunk {sc.chunk_id}: Failed to convert embedding: {e}")
        raise RuntimeError(f"Embedding conversion failed: {e}") from e
```

**Racionalidad**: Si embeddings no se pueden preservar, el sistema DEBE fallar (fail-fast), no continuar con datos degradados.

---

## 5. BUG MEDIO: Quality Gates No Validan provenance_completeness = 1.0

### Ubicaci√≥n
- **Archivo:** `src/saaaaaa/processing/spc_ingestion/quality_gates.py`
- **L√≠neas:** 62-111

### Problema
La clase `SPCQualityGates` valida:
- ‚úÖ Chunk count (MIN_CHUNKS = 5, MAX_CHUNKS = 200)
- ‚úÖ Chunk length (50-5000 chars)
- ‚úÖ Strategic score (>= 0.3)
- ‚úÖ Quality score (>= 0.5)

Pero **NO** valida:
- ‚ùå `provenance_completeness = 1.0` (requerido por README)
- ‚ùå `structural_consistency >= 1.0` (requerido por FASE 3)
- ‚ùå Embeddings presentes y dimensi√≥n correcta
- ‚ùå Causal chains no vac√≠as para chunks DIAGNOSTICO

### Fix de Alto Est√°ndar
```python
class SPCQualityGates:
    # Agregar thresholds
    MIN_PROVENANCE_COMPLETENESS = 1.0  # 100% required
    MIN_STRUCTURAL_CONSISTENCY = 1.0
    MIN_EMBEDDING_DIM = 384

    def validate_quality_metrics(self, quality_metrics: QualityMetrics) -> Dict[str, Any]:
        """
        Validate quality metrics from CanonPolicyPackage.

        Enforces strict thresholds per README specifications.
        """
        failures = []

        # CRITICAL: Provenance completeness must be 100%
        if quality_metrics.provenance_completeness < self.MIN_PROVENANCE_COMPLETENESS:
            failures.append(
                f"Provenance completeness below threshold: "
                f"{quality_metrics.provenance_completeness:.2%} < {self.MIN_PROVENANCE_COMPLETENESS:.0%}"
            )

        # CRITICAL: Structural consistency must be perfect
        if quality_metrics.structural_consistency < self.MIN_STRUCTURAL_CONSISTENCY:
            failures.append(
                f"Structural consistency below threshold: "
                f"{quality_metrics.structural_consistency:.2%} < {self.MIN_STRUCTURAL_CONSISTENCY:.0%}"
            )

        # Check boundary F1
        if quality_metrics.boundary_f1 < 0.85:
            failures.append(f"Boundary F1 too low: {quality_metrics.boundary_f1:.2f} < 0.85")

        # Check budget consistency
        if quality_metrics.budget_consistency_score < 0.95:
            failures.append(
                f"Budget consistency too low: {quality_metrics.budget_consistency_score:.2f} < 0.95"
            )

        return {
            "passed": len(failures) == 0,
            "failures": failures,
        }
```

---

## MATRIZ DE PRIORIDADES PARA FIXES

| Bug | Severidad | Impacto | Esfuerzo | Prioridad |
|-----|-----------|---------|----------|-----------|
| 1. run_normalize() ignora config | üî¥ CR√çTICO | MUY ALTO | 4 horas | P0 |
| 2. Unicode normalization no implementada | üî¥ CR√çTICO | MUY ALTO | 6 horas | P0 |
| 3. Split simplista por \n | üî¥ CR√çTICO | ALTO | 8 horas | P0 |
| 4. SmartChunkConverter silent fail | üü° ALTO | MEDIO | 2 horas | P1 |
| 5. Quality gates incompletos | üü° MEDIO | MEDIO | 3 horas | P1 |

**Tiempo total estimado**: 23 horas (3 d√≠as)

---

## PLAN DE ACCI√ìN INMEDIATO

### D√≠a 1: Fixes Cr√≠ticos de Normalizaci√≥n
1. **Implementar normalizaci√≥n Unicode completa** (6h)
   - NFC/NFKC seg√∫n config
   - Normalizaci√≥n de espacios
   - Eliminaci√≥n opcional de diacr√≠ticos
   - Tests unitarios con casos edge

2. **Implementar segmentaci√≥n de oraciones con spaCy** (4h)
   - Cargar modelo es_core_news_lg
   - Extraer oraciones reales
   - Metadata rica por oraci√≥n
   - Tests con PDFs reales

3. **Integrar config en run_normalize()** (2h)
   - Leer cfg.unicode_form
   - Leer cfg.keep_diacritics
   - Validar precondiciones
   - Logging detallado

### D√≠a 2: Fixes de Data Integrity
4. **Fix SmartChunkConverter fail-fast** (2h)
   - Eliminar try-except silencioso
   - Logging de errores
   - Validaci√≥n de embeddings
   - Tests de failure cases

5. **Extender Quality Gates** (3h)
   - Validar provenance_completeness = 1.0
   - Validar structural_consistency
   - Validar embeddings
   - Tests exhaustivos

### D√≠a 3: Validaci√≥n y Documentaci√≥n
6. **Tests de integraci√≥n** (4h)
   - Pipeline completo con documento real
   - Validar determinismo (10 runs)
   - Validar quality metrics
   - Comparar con baseline

7. **Documentaci√≥n y PR** (2h)
   - Actualizar README
   - Changelog detallado
   - Migration guide
   - PR con evidencia de fixes

---

## VALIDACI√ìN DE M√ÅXIMO EST√ÅNDAR

Para cada fix, validar:

‚úÖ **No downgrades**: Funcionalidad nueva >= funcionalidad anterior
‚úÖ **Complejidad apropiada**: Algoritmos SOTA, no shortcuts
‚úÖ **Determinismo**: Mismo input ‚Üí mismo output byte-a-byte
‚úÖ **Trazabilidad**: Logging completo, provenance 100%
‚úÖ **Fail-fast**: Errores fallan inmediatamente, no se propagan
‚úÖ **Configurabilidad**: Todos los par√°metros expuestos en config
‚úÖ **Tests**: Cobertura >= 95%, edge cases incluidos

---

## REFERENCIAS

- README.md: SPC Phase-One como punto de entrada can√≥nico
- CANONICAL_FLUX.md: Arquitectura determinista del pipeline
- src/saaaaaa/processing/spc_ingestion/__init__.py: CPPIngestionPipeline (√öNICO punto de entrada)
- src/saaaaaa/flux/configs.py l√≠neas 30-46: NormalizeConfig bien dise√±ada
- src/saaaaaa/flux/phases.py: FLUX Phase 2 (normalize) - ten√≠a TODO en producci√≥n (AHORA CORREGIDO)
- src/saaaaaa/processing/spc_ingestion/quality_gates.py: SPCQualityGates (AHORA EXTENDIDO)
- src/saaaaaa/processing/spc_ingestion/converter.py: SmartChunkConverter (AHORA CON FAIL-FAST)

---

**Conclusi√≥n**: Sistema tiene arquitectura clara con SPC como Phase-One can√≥nico. FLUX proporciona fases complementarias. Los bugs cr√≠ticos en FLUX Phase 2 (normalize) y SPC quality gates han sido CORREGIDOS implementando m√°ximo est√°ndar:

‚úÖ **FLUX run_normalize()**: Unicode NFC/NFKC + spaCy sentence segmentation + metadata rica
‚úÖ **SPC SmartChunkConverter**: Fail-fast en p√©rdida de embeddings (no silent failures)
‚úÖ **SPC SPCQualityGates**: Validaci√≥n de provenance_completeness = 1.0 y structural_consistency = 1.0

**Estado**: TODOS LOS BUGS CR√çTICOS CORREGIDOS - Sistema ahora opera en M√ÅXIMO EST√ÅNDAR.
