Fix 2: Executor Integration - From "Passed But Not Used" to "Actually Invoked"
PROBLEM: Audit shows "No self.calibration.calibrate() calls found"
ROOT CAUSE: I gave a template but not EXACT insertion points in existing code
CORRECTED SPECIFICATION:
Step 1: Locate Existing Code
bash# Find the execute method
grep -n "def execute_with_optimization" src/saaaaaa/core/orchestrator/executors.py
# Output will show line number (e.g., line 1420)
Step 2: Insert Calibration Hook - EXACT LOCATION
FILE: src/saaaaaa/core/orchestrator/executors.py
METHOD: AdvancedDataFlowExecutor.execute_with_optimization
LINE NUMBER: After line ~1450 (after parameter setup, before results dict creation)
FIND THIS EXACT CODE (should be around line 1450):
pythondef execute_with_optimization(
    self,
    doc: Any,
    question_id: str,
    dimension_id: str,
    policy_area_id: str,
    method_sets: dict,
    **kwargs
) -> dict:
    """Execute methods with calibration."""
    
    # ... existing parameter validation ...
    
    # FIND THIS LINE:
    results = {}  # <-- THIS IS YOUR INSERTION POINT
INSERT THIS BLOCK immediately BEFORE results = {}:
python    # ============================================================
    # CALIBRATION PHASE (NEW - inserted per corrected spec)
    # ============================================================
    calibration_results = {}
    skipped_methods = []
    
    if self.calibration is not None:
        logger.info("calibration_phase_start", question=question_id)
        
        # Build context
        try:
            from saaaaaa.core.calibration.data_structures import ContextTuple
            
            context = ContextTuple(
                question_id=question_id,
                dimension=dimension_id,
                policy_area=policy_area_id,
                unit_quality=getattr(doc, 'unit_quality', 0.75)
            )
            
            # Calibrate each method
            for method_role, method_list in method_sets.items():
                for method_spec in method_list:
                    method_id = method_spec.get("id", "unknown")
                    method_version = method_spec.get("version", "1.0")
                    
                    try:
                        # THIS IS THE CRITICAL CALL THAT WAS MISSING:
                        cal_result = self.calibration.calibrate(
                            method_id=method_id,
                            method_version=method_version,
                            context=context,
                            pdt_structure=getattr(doc, 'pdt_structure', None),
                            graph_config=self.config.compute_hash(),
                            subgraph_id=f"{question_id}_{method_role}"
                        )
                        
                        calibration_results[method_id] = cal_result
                        
                        logger.info(
                            "method_calibrated",
                            method=method_id,
                            final_score=cal_result.final_score,
                            role=method_role
                        )
                        
                    except Exception as e:
                        logger.error(
                            "calibration_failed",
                            method=method_id,
                            error=str(e),
                            exc_info=True
                        )
                        # Continue without calibration for this method
            
            logger.info("calibration_phase_complete", num_calibrated=len(calibration_results))
            
        except Exception as e:
            logger.error("calibration_phase_error", error=str(e), exc_info=True)
    else:
        logger.info("calibration_disabled", reason="orchestrator_is_none")
    
    # ============================================================
    # END CALIBRATION PHASE
    # ============================================================
    
    results = {}  # <-- EXISTING LINE STAYS HERE
Step 3: Add Method Skipping Logic - EXACT LOCATION
FIND THIS CODE (should be around line 1480, in method execution loop):
python    for method_role, method_list in method_sets.items():
        for method_spec in method_list:
            method_id = method_spec.get("id")
            
            # EXISTING METHOD EXECUTION CODE HERE
            # (whatever is currently there)
INSERT THIS AT THE TOP of the inner for loop (right after method_id = method_spec.get("id")):
python            # ============================================================
            # METHOD SKIPPING BASED ON CALIBRATION (NEW)
            # ============================================================
            if method_id in calibration_results:
                cal_score = calibration_results[method_id].final_score
                
                SKIP_THRESHOLD = 0.3  # From corrected spec
                
                if cal_score < SKIP_THRESHOLD:
                    logger.warning(
                        "method_skipped_low_calibration",
                        method=method_id,
                        score=cal_score,
                        threshold=SKIP_THRESHOLD
                    )
                    
                    skipped_methods.append({
                        "method_id": method_id,
                        "calibration_score": cal_score,
                        "threshold": SKIP_THRESHOLD,
                        "reason": "calibration_score_below_threshold"
                    })
                    
                    continue  # SKIP THIS METHOD
            # ============================================================
            # END METHOD SKIPPING
            # ============================================================
            
            # EXISTING METHOD EXECUTION CODE CONTINUES HERE
Step 4: Add Calibration Results to Output - EXACT LOCATION
FIND THIS CODE (should be at end of execute_with_optimization method):
python    return results  # <-- FIND THIS LINE
INSERT THIS immediately BEFORE the return statement:
python    # ============================================================
    # ADD CALIBRATION RESULTS TO OUTPUT (NEW)
    # ============================================================
    if calibration_results:
        results["_calibration"] = {
            "executed_at": datetime.utcnow().isoformat(),
            "config_hash": self.calibration.config.compute_system_hash() if self.calibration else None,
            "scores": {
                method_id: {
                    "final_score": res.final_score,
                    "layer_breakdown": {
                        layer.value: score.score
                        for layer, score in res.layer_scores.items()
                    },
                    "linear_contribution": res.linear_contribution,
                    "interaction_contribution": res.interaction_contribution,
                    "config_hash": res.computation_metadata.get("config_hash"),
                }
                for method_id, res in calibration_results.items()
            },
            "skipped_methods": skipped_methods,
            "total_methods_calibrated": len(calibration_results),
            "total_methods_skipped": len(skipped_methods),
        }
    # ============================================================
    # END CALIBRATION RESULTS
    # ============================================================
    
    return results  # <-- EXISTING LINE STAYS
Step 5: VERIFICATION SCRIPT - Integration Test
FILE: scripts/verify_executor_integration.py (NEW)
python"""
Verify calibration is actually invoked during executor execution.

This is an INTEGRATION test that proves the fix worked.
"""
import sys
from pathlib import Path
import json

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from saaaaaa.core.orchestrator.executors import D1Q1_Executor
from saaaaaa.core.orchestrator.core import MethodExecutor
from saaaaaa.core.calibration import CalibrationOrchestrator, DEFAULT_CALIBRATION_CONFIG


def test_calibration_is_invoked():
    """Verify calibration is actually called during execution."""
    
    # Setup
    method_exec = MethodExecutor()
    
    # Create calibration orchestrator
    compat_path = Path("data/method_compatibility.json")
    if not compat_path.exists():
        print("⚠️  SKIP: method_compatibility.json not found")
        return True
    
    orch = CalibrationOrchestrator(
        config=DEFAULT_CALIBRATION_CONFIG,
        compatibility_path=compat_path
    )
    
    # Create executor WITH calibration
    executor = D1Q1_Executor(
        method_executor=method_exec,
        signal_registry=None,
        calibration_orchestrator=orch
    )
    
    # Create mock document
    class MockDoc:
        def __init__(self):
            from saaaaaa.core.calibration.pdt_structure import PDTStructure
            self.pdt_structure = PDTStructure(
                full_text="test",
                total_tokens=100
            )
            self.unit_quality = 0.75
    
    doc = MockDoc()
    
    # Execute
    try:
        result = executor.execute_with_optimization(
            doc=doc,
            question_id="Q001",
            dimension_id="DIM01",
            policy_area_id="PA01",
            method_sets={
                "analyzer": [{"id": "pattern_extractor_v2", "version": "v2.1.0"}]
            }
        )
        
        # CRITICAL CHECK 1: _calibration field must exist
        if "_calibration" not in result:
            print("❌ FAIL: _calibration field missing from output")
            print(f"   Output keys: {list(result.keys())}")
            return False
        
        # CRITICAL CHECK 2: scores must be populated
        if "scores" not in result["_calibration"]:
            print("❌ FAIL: _calibration.scores missing")
            return False
        
        scores = result["_calibration"]["scores"]
        if len(scores) == 0:
            print("❌ FAIL: No methods were calibrated")
            return False
        
        # CRITICAL CHECK 3: Verify structure
        first_method = list(scores.keys())[0]
        method_data = scores[first_method]
        
        required_fields = ["final_score", "layer_breakdown", "linear_contribution"]
        for field in required_fields:
            if field not in method_data:
                print(f"❌ FAIL: Missing field '{field}' in calibration output")
                return False
        
        print(f"✅ PASS: Calibration is integrated and working")
        print(f"   Calibrated {len(scores)} method(s)")
        print(f"   Skipped {result['_calibration'].get('total_methods_skipped', 0)} method(s)")
        print(f"   Sample score: {method_data['final_score']:.3f}")
        
        return True
        
    except Exception as e:
        print(f"❌ FAIL: Execution error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_calibration_is_invoked()
    sys.exit(0 if success else 1)
RUN THIS:
bashpython3 scripts/verify_executor_integration.py
# MUST output: ✅ PASS: Calibration is integrated and working
